# GMAI-VL & GMAI-VL-5.5M: A General Medical Vision-Language Model and Multimodal Dataset

Welcome to the GMAI-VL code repository, which accompanies the paper "GMAI-VL & GMAI-VL-5.5M: A General Medical Vision-Language Model and Multimodal Dataset." This repository provides the resources needed for reproducing the results and furthering research in medical AI through vision-language models.
This repository includes:

- **GMAI-VL**: A state-of-the-art general medical vision-language model.
- **GMAI-VL-5.5M**: A comprehensive multimodal medical dataset containing 5.5 million images and associated text, designed to support a wide range of medical AI research.

## ðŸš§ Coming Soon: Code, Dataset, and Model Weights ðŸš§

We are currently organizing and preparing the following resources for public release:

- **Code**: Full implementation of the GMAI-VL model, including training and evaluation scripts.
- **Dataset**: GMAI-VL-5.5M, a large-scale multimodal medical dataset.
- **Model Weights**: Model weights of our model GMAI-VL.

Stay tuned for upcoming updates!

## ðŸ“… Release Timeline

- **11.21.2024**: The paper was officially released on [arXiv](https://arxiv.org/abs/2411.14522)!
- **Coming Soon**: Code, dataset, and model weights will be released. Please watch this repository for updates. We are committed to making these resources available as soon as possible. Please watch this repository or check back regularly for updates.

## ðŸ”— Stay Connected

For inquiries, collaboration opportunities, or access requests, feel free to reach out via email or open a GitHub issue.

Thank you for your interest and support!
## ðŸ“„ Paper and Citation

Our paper has been published on [arXiv](https://arxiv.org/abs/2411.14522). If you use our work in your research, please consider citing us:

### BibTeX Citation
```bibtex
@article{li2024gmai,
      title={GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI},
      author={Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, Yanjun Li, Pengcheng Chen, Xiaowei Hu, Zhongying Deng, Yuanfeng Ji, Jin Ye, Yu Qiao, Junjun He},
  journal={arXiv preprint arXiv:2411.14522},
  year={2024}
}