Metadata-Version: 2.4
Name: xtuner
Version: 0.1.21
Summary: An efficient, flexible and full-featured toolkit for fine-tuning large models
Home-page: https://github.com/InternLM/xtuner
Author: XTuner Contributors
Author-email: openmmlab@gmail.com
License: Apache License 2.0
Keywords: large language model,parameter-efficient fine-tuning
Classifier: Development Status :: 4 - Beta
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Utilities
Requires-Python: >=3.8, <3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: bitsandbytes>=0.40.0.post4
Requires-Dist: datasets>=2.16.0
Requires-Dist: einops
Requires-Dist: lagent>=0.1.2
Requires-Dist: mmengine>=0.10.1
Requires-Dist: openpyxl
Requires-Dist: peft>=0.4.0
Requires-Dist: scipy
Requires-Dist: SentencePiece
Requires-Dist: tiktoken
Requires-Dist: torch<2.8
Requires-Dist: transformers!=4.34.1,!=4.35.0,!=4.35.1,!=4.35.2,<4.47,>=4.32.1
Requires-Dist: loguru
Requires-Dist: pydantic
Provides-Extra: all
Requires-Dist: bitsandbytes>=0.40.0.post4; extra == "all"
Requires-Dist: datasets>=2.16.0; extra == "all"
Requires-Dist: einops; extra == "all"
Requires-Dist: lagent>=0.1.2; extra == "all"
Requires-Dist: mmengine>=0.10.1; extra == "all"
Requires-Dist: openpyxl; extra == "all"
Requires-Dist: peft>=0.4.0; extra == "all"
Requires-Dist: scipy; extra == "all"
Requires-Dist: SentencePiece; extra == "all"
Requires-Dist: tiktoken; extra == "all"
Requires-Dist: torch<2.8; extra == "all"
Requires-Dist: transformers!=4.34.1,!=4.35.0,!=4.35.1,!=4.35.2,<4.47,>=4.32.1; extra == "all"
Requires-Dist: loguru; extra == "all"
Requires-Dist: pydantic; extra == "all"
Requires-Dist: deepspeed>=0.12.3; extra == "all"
Requires-Dist: mpi4py-mpich; extra == "all"
Requires-Dist: modelscope; extra == "all"
Provides-Extra: deepspeed
Requires-Dist: bitsandbytes>=0.40.0.post4; extra == "deepspeed"
Requires-Dist: datasets>=2.16.0; extra == "deepspeed"
Requires-Dist: einops; extra == "deepspeed"
Requires-Dist: lagent>=0.1.2; extra == "deepspeed"
Requires-Dist: mmengine>=0.10.1; extra == "deepspeed"
Requires-Dist: openpyxl; extra == "deepspeed"
Requires-Dist: peft>=0.4.0; extra == "deepspeed"
Requires-Dist: scipy; extra == "deepspeed"
Requires-Dist: SentencePiece; extra == "deepspeed"
Requires-Dist: tiktoken; extra == "deepspeed"
Requires-Dist: torch<2.8; extra == "deepspeed"
Requires-Dist: transformers!=4.34.1,!=4.35.0,!=4.35.1,!=4.35.2,<4.47,>=4.32.1; extra == "deepspeed"
Requires-Dist: loguru; extra == "deepspeed"
Requires-Dist: pydantic; extra == "deepspeed"
Requires-Dist: deepspeed>=0.12.3; extra == "deepspeed"
Requires-Dist: mpi4py-mpich; extra == "deepspeed"
Provides-Extra: modelscope
Requires-Dist: bitsandbytes>=0.40.0.post4; extra == "modelscope"
Requires-Dist: datasets>=2.16.0; extra == "modelscope"
Requires-Dist: einops; extra == "modelscope"
Requires-Dist: lagent>=0.1.2; extra == "modelscope"
Requires-Dist: mmengine>=0.10.1; extra == "modelscope"
Requires-Dist: openpyxl; extra == "modelscope"
Requires-Dist: peft>=0.4.0; extra == "modelscope"
Requires-Dist: scipy; extra == "modelscope"
Requires-Dist: SentencePiece; extra == "modelscope"
Requires-Dist: tiktoken; extra == "modelscope"
Requires-Dist: torch<2.8; extra == "modelscope"
Requires-Dist: transformers!=4.34.1,!=4.35.0,!=4.35.1,!=4.35.2,<4.47,>=4.32.1; extra == "modelscope"
Requires-Dist: loguru; extra == "modelscope"
Requires-Dist: pydantic; extra == "modelscope"
Requires-Dist: modelscope; extra == "modelscope"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# GMAI-VL & GMAI-VL-5.5M: A General Medical Vision-Language Model and Multimodal Dataset

Welcome to the GMAI-VL code repository, which accompanies the paper "GMAI-VL & GMAI-VL-5.5M: A General Medical Vision-Language Model and Multimodal Dataset." This repository provides the resources needed for reproducing the results and furthering research in medical AI through vision-language models.
This repository includes:

- **GMAI-VL**: A state-of-the-art general medical vision-language model.
- **GMAI-VL-5.5M**: A comprehensive multimodal medical dataset containing 5.5 million images and associated text, designed to support a wide range of medical AI research.


## üõ†Ô∏è Model Training Instructions

### 1. Installation

To set up the environment for training, please follow the instructions in the [xtuner repository](https://github.com/InternLM/xtuner). Ensure that all dependencies are correctly installed.
```bash
git clone https://github.com/uni-medical/GMAI-VL
cd GMAI-VL
pip install -e .
```
### 2. Data Preparation

Prepare your training data in the format shown in examples/example_data.json. To support multiple JSON datasets with different sampling ratios, use the format defined in examples/example_list.json.

Example structure for example_list.json:
```
{
    "FILE1": {
        "image_dir": "",
        "annotations": "examples/example_data.json",
        "sample_ratio": 1.0,
        "length": 38276,
        "data_augment": true
    },
    ...
}
```
### 3. Training

Here is a reference script to start training:

```bash
SRUN_ARGS=${SRUN_ARGS:-""}

export PYTHONPATH="$(pwd):$(pwd)/../"
export MASTER_PORT=34220


export NPROC_PER_NODE=${KUBERNETES_CONTAINER_RESOURCE_GPU}
export PORT=${MASTER_PORT}
export NNODES=${WORLD_SIZE}
export NODE_RANK=${RANK}
export ADDR=${MASTER_ADDR}
export HF_HOME=~/.cache
export USE_TRITON_KERNEL=1

torchrun --nnodes=${WORLD_SIZE} --node_rank=${RANK} --nproc_per_node=${NPROC_PER_NODE} --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
    tools/llava/llava_sft.py \
    --freeze-llm \
    --freeze-vit \
    --llava $base_model_path/ \
    --chat-template internlm2 \
    --datasets examples/examples_list.json \
    --num-workers 6 \
    --mirco-batch-size $MIRCO_BATCH_SIZE \
    --global-batch-size $GLOBAL_BATCH_SIZE \
    --lr 1e-5 \
    --wd 0 \
    --dset-pack-level soft \
    --shard-strategy 'zero2' \
    --group-by-length \
    --resume \
    --max-length 2048 \
    --checkpoint-interval 500 \
    --log-interval 10 \
    --work-dir $work_dir/ \
    --dset-cache-dir $work_dir/cache/ \
    --dset-from-cache
  ```
üí° Note: Our training follows a multi-stage strategy. At each stage, different components (e.g., LLM or ViT) may be frozen or fine-tuned. Please adjust flags such as --freeze-llm, --freeze-vit, and learning rates accordingly, as described in the paper.
## üìä Evaluation
For evaluation, please use the [VLMEvalKit](https://github.com/open-compass/VLMEvalKit). It provides comprehensive tools for evaluating vision-language models on various tasks.

## üìÖ Release Timeline

- **2025-04-02**: The training code and model weight for the GMAI-VL model has been officially released! üéâ  
This update includes detailed instructions for model training, dataset preparation, and evaluation.
- **2024-11-21**: The paper was officially released on [arXiv](https://arxiv.org/abs/2411.14522)!
- **Coming Soon**: dataset will be released. Please watch this repository for updates. We are committed to making these resources available as soon as possible. Please watch this repository or check back regularly for updates.

## üîó Stay Connected

For inquiries, collaboration opportunities, or access requests, feel free to reach out via email or open a GitHub issue.

Thank you for your interest and support!

## üôè Acknowledgements

We would like to express our sincere gratitude to the open-source community. Our work is built upon the excellent contributions of the following toolkits:

- **[XTuner](https://github.com/InternLM/xtuner)**: An efficient, flexible, and full-featured toolkit for fine-tuning large language and vision-language models. It served as the core training framework for GMAI-VL.
- **[VLMEvalKit](https://github.com/open-compass/VLMEvalKit)**: A comprehensive evaluation toolkit for large vision-language models (LVLMs). It enabled us to conduct rigorous and standardized evaluations across multiple benchmarks.

We deeply appreciate the efforts of the developers and contributors behind these projects.

## üìÑ Paper and Citation

Our paper has been published on [arXiv](https://arxiv.org/abs/2411.14522). If you use our work in your research, please consider citing us:

### BibTeX Citation
```bibtex
@article{li2024gmai,
      title={GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI},
      author={Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, Yanjun Li, Pengcheng Chen, Xiaowei Hu, Zhongying Deng, Yuanfeng Ji, Jin Ye, Yu Qiao, Junjun He},
  journal={arXiv preprint arXiv:2411.14522},
  year={2024}
}
